{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of flair_POS",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYxao0TVGxqH"
      },
      "source": [
        "Make sure runtime is GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J49iMHkXp2M",
        "outputId": "12e4be8b-92e6-47cd-c7c9-55f36a9b0d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DQKkEYIzli3",
        "outputId": "bdf6cd20-88d3-453b-859e-00d7b9e2af9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install flair\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from flair.models import SequenceTagger\n",
        "SequenceTagger.load('chunk')\n",
        "SequenceTagger.load('pos')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/19/902d1691c1963ab8c9a9578abc2d65c63aa1ecf4f8200143b5ef91ace6f5/flair-0.6.1-py3-none-any.whl (331kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 4.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 10.2MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/36/9e022b76a3ac440e1d750c64fa6152469f988efe0c568b945e396e2693b5/pytest-6.1.1-py3-none-any.whl (272kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 31.8MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 49.0MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Collecting transformers>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.15.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (2.0.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.4)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown->flair) (2.23.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.18.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2020.6.20)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=3.0.0->flair) (50.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: langdetect, segtok, mpld3, ftfy, sqlitedict, overrides, sacremoses\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=c2d22fcaa565a0635e56e8b5178a75d99ee29ac045c2aeaf45a282a704e6e4bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=691d00b29c281c4708ba0ca361ac92fe8fb36df3f86f635e03315e02d3a563db\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=0da6895337f8b4ba8e0763690ce7d8ece21daad81715be72687fc4a0e377976a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=d237014ceb91c0f35de089c883ff88a5bd70d7a3cebc0fabc122db3620fee27a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14377 sha256=ad6e027631d31ec5f27dfae3b9304cd4fbc504fb45c9387ac54b35a559091b3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=7aa3fc364ea324e3f3568de81ed7a2f706c4d1445141748679753468451566a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3dbd6f4615649cad1ae892aac38af34c5ee179f274790a7fb78ed873564951b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built langdetect segtok mpld3 ftfy sqlitedict overrides sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, deprecated, overrides, konoha, langdetect, pluggy, pytest, janome, segtok, mpld3, ftfy, sqlitedict, tokenizers, sacremoses, transformers, bpemb, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.2 deprecated-1.2.10 flair-0.6.1 ftfy-5.8 janome-0.4.1 konoha-4.6.2 langdetect-1.0.8 mpld3-0.3 overrides-3.0.0 pluggy-0.13.1 pytest-6.1.1 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.94 sqlitedict-1.7.0 tokenizers-0.9.2 transformers-3.4.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "2020-10-24 19:07:55,919 https://nlp.informatik.hu-berlin.de/resources/models/chunk/en-chunk-conll2000-v0.4.pt not found in cache, downloading to /tmp/tmpnk_y9mnq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 249034168/249034168 [00:40<00:00, 6130504.33B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-24 19:08:37,015 copying /tmp/tmpnk_y9mnq to cache at /root/.flair/models/en-chunk-conll2000-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-24 19:08:37,544 removing temp file /tmp/tmpnk_y9mnq\n",
            "2020-10-24 19:08:38,349 loading file /root/.flair/models/en-chunk-conll2000-v0.4.pt\n",
            "2020-10-24 19:08:53,176 https://nlp.informatik.hu-berlin.de/resources/models/pos/en-pos-ontonotes-v0.5.pt not found in cache, downloading to /tmp/tmpi84mrec4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 249072763/249072763 [00:15<00:00, 16165666.10B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-24 19:09:09,058 copying /tmp/tmpi84mrec4 to cache at /root/.flair/models/en-pos-ontonotes-v0.5.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-24 19:09:09,714 removing temp file /tmp/tmpi84mrec4\n",
            "2020-10-24 19:09:09,773 loading file /root/.flair/models/en-pos-ontonotes-v0.5.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceTagger(\n",
              "  (embeddings): StackedEmbeddings(\n",
              "    (list_embedding_0): FlairEmbeddings(\n",
              "      (lm): LanguageModel(\n",
              "        (drop): Dropout(p=0.05, inplace=False)\n",
              "        (encoder): Embedding(300, 100)\n",
              "        (rnn): LSTM(100, 2048)\n",
              "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (list_embedding_1): FlairEmbeddings(\n",
              "      (lm): LanguageModel(\n",
              "        (drop): Dropout(p=0.05, inplace=False)\n",
              "        (encoder): Embedding(300, 100)\n",
              "        (rnn): LSTM(100, 2048)\n",
              "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (word_dropout): WordDropout(p=0.05)\n",
              "  (locked_dropout): LockedDropout(p=0.5)\n",
              "  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)\n",
              "  (linear): Linear(in_features=512, out_features=53, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r--gB2PDdRHX"
      },
      "source": [
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNCGDtBEY_-3"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from nltk import tokenize, sent_tokenize\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import logging"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAZcz0RfEXOw"
      },
      "source": [
        "# Filtering\n",
        "1. First use chunking to find sentences that have at least three noun phrases (A, B, pronoun)\n",
        "2. Then use POS tagging to find sentences that have the specific pronouns you want\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_HNqlOa_v6s",
        "outputId": "cd78d976-6f7a-45df-f6d6-bb821937a2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "tagger = SequenceTagger.load('chunk')\n",
        "ner_tagger = SequenceTagger.load('ner')\n",
        "pos_tagger = SequenceTagger.load('pos')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-24 19:09:23,880 loading file /root/.flair/models/en-chunk-conll2000-v0.4.pt\n",
            "2020-10-24 19:09:25,175 https://nlp.informatik.hu-berlin.de/resources/models/ner/en-ner-conll03-v0.4.pt not found in cache, downloading to /tmp/tmpajheaui6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 432197603/432197603 [00:24<00:00, 17663784.10B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-24 19:09:50,118 copying /tmp/tmpajheaui6 to cache at /root/.flair/models/en-ner-conll03-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-24 19:09:51,487 removing temp file /tmp/tmpajheaui6\n",
            "2020-10-24 19:09:51,534 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n",
            "2020-10-24 19:09:53,241 loading file /root/.flair/models/en-pos-ontonotes-v0.5.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOn8B8iQ077D"
      },
      "source": [
        "def get_sentences(raw_sentences,tagger,ner_tagger,pos_tagger,search_pattern):\n",
        "  # print('Tokenizing sentences for filter 1')\n",
        "  sentences = [Sentence(s) for s in raw_sentences]\n",
        "\n",
        "  # Chunking the sentences into phrases\n",
        "  # tagger = SequenceTagger.load('chunk')\n",
        "  # print(\"Chunking sentences\")\n",
        "  tagger.predict(sentences)\n",
        "\n",
        "  # Count the number of noun phrases. There should be at least 3 (A, B, pronoun)\n",
        "  # logging.info(\"\\nApplying filter 1\")\n",
        "  filter1_sentences = []\n",
        "  for sent in sentences:\n",
        "      if [chunk.tag for chunk in sent.get_spans()].count('NP') >= 3:\n",
        "          filter1_sentences.append(sent.to_original_text())\n",
        "  # print('\\nFilter 1: Removed {} sentences'.format(len(sentences) - len(filter1_sentences)))\n",
        "\n",
        "  # print(\"Tokenizing sentences for NER filter\")\n",
        "  sentences = [Sentence(s) for s in filter1_sentences]\n",
        "  # ner_tagger = SequenceTagger.load('ner')\n",
        "  ner_tagger.predict(sentences)\n",
        "\n",
        "  # logging.info(\"\\nApplying filter NER\")\n",
        "  NER_sentences = []\n",
        "  for sent in sentences:\n",
        "      if [entity.tag for entity in sent.get_spans('ner')].count('PER') >= 2:\n",
        "        for entity in sent.get_spans('ner'):\n",
        "          if entity.tag == 'PER' and len(entity)> 1:\n",
        "            original_name = entity.text\n",
        "            new_name = str(entity.text).split(' ')[0] # gets the first name\n",
        "            sent = sent.to_original_text()\n",
        "            sent = Sentence(sent.replace(original_name, new_name)) #Replaces the name and re-tokenizes the sentence\n",
        "        \n",
        "        NER_sentences.append(sent.to_original_text())\n",
        "\n",
        "  # print('\\n Sentences after Filter 1: {}'.format(len(NER_sentences)))\n",
        "  # print(\"Tokenizing sentences for filter 2\")\n",
        "  sentences = [Sentence(s) for s in NER_sentences]\n",
        "  len(sentences)\n",
        "  # POS tagging\n",
        "  # pos_tagger = SequenceTagger.load('pos')\n",
        "  # print(\"Tagging sentences with POS\")\n",
        "  pos_tagger.predict(sentences)\n",
        "\n",
        "  # Only select sentences with one gendered pronoun and no other pronouns\n",
        "  # print(\"\\nApplying filter 2\")\n",
        "  filter2_sentences = []\n",
        "  for sent in sentences:\n",
        "      token_list = [token.text for token in sent]\n",
        "      if (token_list.count('he') + token_list.count('she') == 1) and [token.get_tag('pos').value for token in sent].count('PRP') == 1:\n",
        "          filter2_sentences.append(sent.to_original_text())\n",
        "  print(\"len after filter 2 is {}\".format(len(filter2_sentences)))\n",
        "\n",
        "  filter3 = []\n",
        "  for sent in filter2_sentences:\n",
        "    r = search_pattern.search(sent)\n",
        "    if r:\n",
        "      print(\"RE match found {}\".format(r.start()))\n",
        "      sentence = Sentence(sent[:r.start()])\n",
        "      ner_tagger.predict(sentence)\n",
        "      if [entity.tag for entity in sentence.get_spans('ner')].count('PER') >= 1:\n",
        "        filter3.append(sent)\n",
        "\n",
        "\n",
        "  # print('\\nFilter 2: Removed {} sentences'.format(len(sentences) - len(filter2_sentences)))\n",
        "  print(\"length after filter 3 is {}\".format(len(filter3)))\n",
        "  return filter3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuviFoM1-zm9"
      },
      "source": [
        "wiki_san = pd.read_csv('/gdrive/My Drive/NLP-datasets/movie_reviews.csv',names=['Article','Content','link'],encoding='cp1252')\n",
        "wiki_san['Content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3UPOmVuXr2u"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Extract He/she"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZhObuDADQvr"
      },
      "source": [
        "# Get raw sentences from the scrape, removes non-ASCII characters, fixes the full-stop issue \n",
        "filter2_sentences_extract = []\n",
        "total_sentences = 0\n",
        "search_pattern = re.compile(r\"( he | she )\")\n",
        "stop_regex = re.compile(r\"\\.\")\n",
        "no_bracket_regex = re.compile(r\"[\\(\\[].*?[\\)\\]]\")\n",
        "for para in tqdm(wiki_san['Content']):\n",
        "  try:\n",
        "    para = stop_regex.sub(\" . \", para) # replaces space before and after a full-stop\n",
        "    para = re.sub(r'[^\\x00-\\x7f]',r' ',para) # replaces Non-ASCII character with space\n",
        "    para = no_bracket_regex.sub(\" \", para) # replaces content in brackets including the brackets with space\n",
        "    para = re.sub(' +', ' ', para) # replace multiple-space with single space\n",
        "    sentences = sent_tokenize(para)\n",
        "    total_sentences += len(sentences)\n",
        "    filter2_sentences_extract.append(get_sentences(sentences,tagger,ner_tagger,pos_tagger,search_pattern))\n",
        "  except:\n",
        "    print(para)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSRBj7B-YYGs"
      },
      "source": [
        "  def write_to_drive(file_name,filter2_sentences_extract):\n",
        "    processed_sentences = []\n",
        "    for item in filter2_sentences_extract:\n",
        "      if item == []:\n",
        "        continue\n",
        "      else:\n",
        "        for sent in item:\n",
        "          processed_sentences.append(sent)\n",
        "    print(len(processed_sentences))\n",
        "    df = pd.DataFrame({'filtered_sent':processed_sentences})\n",
        "    df.to_csv('/gdrive/My Drive/NLP-datasets/'+file_name)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5iq1czhzjfz",
        "outputId": "f969ad7d-c3e2-4437-a7b9-7157c6699819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "write_to_drive('movie_extracted.csv',filter2_sentences_extract)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyqUBtWxLY3y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}