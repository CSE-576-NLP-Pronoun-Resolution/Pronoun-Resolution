{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Fine-Tuning BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Install Transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkhc10wNrGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74c3e99-b2a1-48e1-8f3a-43cfc6807626"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 29.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b928f76ce17bcb8cac2b3879fa0a461a6c338faea37519ff56471bdc181160b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of_3Jc212nER",
        "outputId": "8787ef0a-cc34-4727-c85d-a44f01785d2f"
      },
      "source": [
        "df_cleaned = pd.read_csv('/content/Cleaned_Dataset.csv')\n",
        "print(df_cleaned.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'A-Coref',\n",
            "       'B', 'B-offset', 'B-Coref', 'Unnamed: 10'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJgOdrEDaIRj"
      },
      "source": [
        "df_cleaned = df_cleaned[df_cleaned['Pronoun'].isin(['he','she'])]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mgOf7oz37NB",
        "outputId": "ccce67de-1f26-42ef-855d-9ec44ecce18b"
      },
      "source": [
        "df_winnow = pd.read_json('/content/winnow_train_s.jsonl',lines=True)\n",
        "print(df_winnow.columns)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['qID', 'sentence', 'option1', 'option2', 'answer'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HARvpDKgZd0b",
        "outputId": "7c6365d8-af38-4d6c-bd1b-3d953f47da4b"
      },
      "source": [
        "print(df_winnow['sentence'][5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kyle doesn't wear leg warmers to bed, while Logan almost always does. _ is more likely to live in a warmer climate.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4XZMcsAW2D5"
      },
      "source": [
        "for idx, row in df_winnow.iterrows():\n",
        "  if '_' in row['sentence']:\n",
        "    sent = row['sentence'].split('.')\n",
        "    sent_single = \" \".join(sent)\n",
        "    sent_single +=\".\"\n",
        "    if idx %2 == 0:\n",
        "      df_winnow.at[idx,'sentence2'] = sent_single.replace('_', 'he')\n",
        "    else:\n",
        "      df_winnow.at[idx,'sentence2'] = sent_single.replace('_', 'she')\n",
        "  else:\n",
        "    print(sent_single)\n",
        "    df_winnow.at[idx,'sentence2'] = sent_single\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC2e7CrcFGjs",
        "outputId": "1d085e01-8968-46c4-8600-ef30206047fb"
      },
      "source": [
        "print(df_winnow['sentence2'][5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kyle doesn't wear leg warmers to bed, while Logan almost always does  she is more likely to live in a warmer climate .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzPPOrVQWiW5"
      },
      "source": [
        "for idx, rows in df_cleaned.iterrows():\n",
        "  if rows['A-Coref'] == True:\n",
        "    df_cleaned.loc[idx,'answer'] = 0\n",
        "  else:\n",
        "    df_cleaned.loc[idx,'answer'] = 1\n",
        "\n",
        "df_cleaned['answer'] = df_cleaned['answer'].astype(np.int64)\n",
        "df_winnow['answer'] = df_winnow['answer'] - 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDvVeJPw5abh"
      },
      "source": [
        "Win_X_train, Win_X_test, Win_y_train, win_y_test = train_test_split(df_winnow['sentence'], df_winnow['answer'],\n",
        "                                                    stratify=df_winnow['answer'], \n",
        "                                                    test_size=0.80)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRF3IjWDFU3o",
        "outputId": "412b1483-0c22-437f-8c20-e160ccef799d"
      },
      "source": [
        "print(len(Win_X_train), len(Win_X_test), len(Win_y_train), len(win_y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128 512 128 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rux4IoNo6_al",
        "outputId": "7dce3e13-a8fe-498b-8cf9-77d6f4871534"
      },
      "source": [
        "cleaned_train_data = df_cleaned[['Text','answer']]\n",
        "cleaned_train_data = cleaned_train_data.rename(columns={'Text':'sentence'})\n",
        "print(len(cleaned_train_data))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "405WTIIXG9Ju"
      },
      "source": [
        "final_data = pd.DataFrame()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWqDk-u387Mq",
        "outputId": "b1c075f2-ed76-4ebd-f3be-1362999b19cd"
      },
      "source": [
        "final_data['sentence'] = cleaned_train_data['sentence'].append(Win_X_train,ignore_index=True)\n",
        "final_data['answer'] = cleaned_train_data['answer'].append(Win_y_train,ignore_index=True)\n",
        "print(len(final_data['sentence']),len(final_data['answer']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6272 6272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "source": [
        "train_text, val_text, train_labels, val_labels = train_test_split(final_data['sentence'], final_data['answer'], \n",
        "                                                                    random_state=2020, \n",
        "                                                                    test_size=0.2, \n",
        "                                                                    stratify=final_data['answer'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvBkViMxCJHf"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "0dcf78aa-afc7-4433-9d1c-67a0664d94c9"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe6ef5dc048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXcswEIRPvGe"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f83b59-7a53-4540-b6cc-903d24ba4d75"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "   train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    Win_X_test.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(win_y_test.tolist())"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZ0MC00RQA_"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bblPV0gyCekP"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJUW1aA9Ci25"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXS0IilRn9J"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 0.001)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izY5xH5eR7Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0417bcd-d97e-41d7-bc4e-742d7b3d8c36"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.75420926 1.48344175]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 25"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      # elapsed = time.time() - t0\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "      # print(preds,labels)\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ee29f0-e50c-4fcc-b21c-d4f9a522eb50"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/bertsaved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.720\n",
            "Validation Loss: 0.699\n",
            "\n",
            " Epoch 2 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 3 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.694\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 4 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 5 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.692\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 6 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 7 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.692\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 8 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.691\n",
            "Validation Loss: 0.694\n",
            "\n",
            " Epoch 9 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.691\n",
            "Validation Loss: 0.692\n",
            "\n",
            " Epoch 10 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.690\n",
            "Validation Loss: 0.698\n",
            "\n",
            " Epoch 11 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.690\n",
            "Validation Loss: 0.691\n",
            "\n",
            " Epoch 12 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.689\n",
            "Validation Loss: 0.691\n",
            "\n",
            " Epoch 13 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.696\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 14 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.690\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 15 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.688\n",
            "Validation Loss: 0.691\n",
            "\n",
            " Epoch 16 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.687\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 17 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.684\n",
            "Validation Loss: 0.689\n",
            "\n",
            " Epoch 18 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.684\n",
            "Validation Loss: 0.689\n",
            "\n",
            " Epoch 19 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.685\n",
            "Validation Loss: 0.689\n",
            "\n",
            " Epoch 20 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.683\n",
            "Validation Loss: 0.687\n",
            "\n",
            " Epoch 21 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.682\n",
            "Validation Loss: 0.693\n",
            "\n",
            " Epoch 22 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.680\n",
            "Validation Loss: 0.686\n",
            "\n",
            " Epoch 23 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.681\n",
            "Validation Loss: 0.690\n",
            "\n",
            " Epoch 24 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.685\n",
            "Validation Loss: 0.710\n",
            "\n",
            " Epoch 25 / 25\n",
            "  Batch    50  of    157.\n",
            "  Batch   100  of    157.\n",
            "  Batch   150  of    157.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.680\n",
            "Validation Loss: 0.686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacxUyizS8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416135a1-2258-4747-89d9-556b8cef9b56"
      },
      "source": [
        "#load weights of best model\n",
        "path = '/content/bertsaved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1ObHZxTYSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c4073b-c0b3-4a01-d856-354e9005e1ce"
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.91      0.64       256\n",
            "           1       0.45      0.07      0.13       256\n",
            "\n",
            "    accuracy                           0.49       512\n",
            "   macro avg       0.47      0.49      0.38       512\n",
            "weighted avg       0.47      0.49      0.38       512\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpX1uTwjUPY6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}